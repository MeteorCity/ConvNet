{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from skimage.util import view_as_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and labels into numpy arrays\n",
    "\n",
    "train_labels_and_data = pd.read_csv(\"/content/sample_data/mnist_train.csv\").to_numpy()\n",
    "test_labels_and_data = pd.read_csv(\"/content/sample_data/mnist_test.csv\").to_numpy()\n",
    "\n",
    "train_labels = train_labels_and_data[5000:, :1]\n",
    "val_labels = train_labels_and_data[:5000, :1]\n",
    "test_labels = test_labels_and_data[:, :1]\n",
    "\n",
    "train_data = np.reshape(train_labels_and_data[5000:, 1:], (55000, 28, 28)) / 255.0\n",
    "val_data = np.reshape(train_labels_and_data[:5000, 1:], (5000, 28, 28)) / 255.0\n",
    "test_data = np.reshape(test_labels_and_data[:, 1:], (10000, 28, 28)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, num_filters, filter_size, learning_rate):\n",
    "        # Originally contained stride_size and padding but were removed for simplicity\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size # filter_size is passed in as an int. The true size is filter_size x filter_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.filter_weights = None\n",
    "        self.biases = np.zeros(num_filters)\n",
    "        self.weights_initialized = False\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "\n",
    "    def convolve(self, input_arr, kernel, fm_rows, fm_cols):\n",
    "        \"\"\"\n",
    "        input_arr: The matrix which we're convolving over\n",
    "        kernel: The filter that is being convolved over input_arr\n",
    "        fm_rows: The number of rows the feature map output will contain\n",
    "        fm_cols: The number of columns the feature map output will contain\n",
    "\n",
    "        Returns an array representing the result of convolution\n",
    "        \"\"\"\n",
    "        assert input_arr.ndim == 3, \"input_arr must be a 3D array (batch_size, height, width)\"\n",
    "        assert kernel.ndim == 3, \"kernel must be a 3D array (batch_size, kernel_height, kernel_width)\"\n",
    "        assert input_arr.shape[1] == input_arr.shape[2], \"All images/feature maps must be square\"\n",
    "        assert input_arr.shape[0] == kernel.shape[0], \"Input and kernel must have the same batch size or in the case of backwards pass, number of filters\"\n",
    "\n",
    "        im2col_matrix = self.im2col(input_arr, kernel, fm_rows, fm_cols)\n",
    "        dot_result = np.dot(kernel.flatten(), im2col_matrix)\n",
    "        conv_result = np.reshape(dot_result, (fm_rows, fm_cols))\n",
    "\n",
    "        return conv_result\n",
    "\n",
    "    def im2col(self, input_arr, kernel, fm_rows, fm_cols):\n",
    "        output_shape_rows = kernel.flatten().shape[0]\n",
    "        output_shape_cols = fm_rows * fm_cols\n",
    "        windows = view_as_windows(input_arr, kernel.shape)\n",
    "        im2row_matrix = np.reshape(windows, (output_shape_cols, output_shape_rows))\n",
    "\n",
    "        return im2row_matrix.T\n",
    "\n",
    "    # Assumes stride_size and filter_size are compatible with the input shape\n",
    "    # Also assumes that input_arr is a three dimensional array. The first dimension being\n",
    "    # the number of feature maps or images, the second and third representing the feature map or image\n",
    "    def forward_pass(self, input_arr):\n",
    "        self.last_input = input_arr\n",
    "\n",
    "        # Initialize filter weights\n",
    "        if not self.weights_initialized:\n",
    "            fan_in = input_arr.shape[0] * input_arr.shape[1] * input_arr.shape[2]\n",
    "            self.filter_weights = np.random.randn(self.num_filters, input_arr.shape[0], self.filter_size, self.filter_size) * np.sqrt(2.0 / fan_in)\n",
    "            self.weights_initialized = True\n",
    "\n",
    "        # Calculate output dimensions and create output array\n",
    "        feature_map_num_rows = (input_arr.shape[1] - self.filter_size) + 1\n",
    "        feature_map_num_cols = (input_arr.shape[2] - self.filter_size) + 1\n",
    "        feature_maps = np.zeros((self.num_filters, feature_map_num_rows, feature_map_num_cols))\n",
    "\n",
    "        for n in range(self.num_filters):\n",
    "            feature_maps[n] = self.convolve(input_arr, self.filter_weights[n], feature_map_num_rows, feature_map_num_cols)\n",
    "\n",
    "            # Add the filter bias to the feature map\n",
    "            feature_maps[n] += self.biases[n]\n",
    "\n",
    "        # Use ReLU activation function\n",
    "        feature_maps = self.relu(feature_maps)\n",
    "\n",
    "        self.last_output = feature_maps\n",
    "        return feature_maps\n",
    "\n",
    "    # Good article is https://pavisj.medium.com/convolutions-and-backpropagations-46026a8f5d2c\n",
    "    # Another good article https://hideyukiinada.github.io/cnn_backprop_strides2.html\n",
    "    # Another good article https://deeplearning.cs.cmu.edu/F21/document/recitation/Recitation5/CNN_Backprop_Recitation_5_F21.pdf\n",
    "    # prev_layer_grad represents the gradient of the loss with respect to each element of the output feature map.\n",
    "    # prev_layer_grad is a numpy array of shape of the last output of feature_maps\n",
    "    def backward_pass(self, prev_layer_grad):\n",
    "        # Apply ReLU derivative to get the pre-activation values\n",
    "        prev_layer_grad *= self.relu_derivative(self.last_output)\n",
    "\n",
    "        # Calculate gradients\n",
    "        filter_gradients = self.calc_filter_grads(prev_layer_grad)\n",
    "        input_gradients = self.calc_input_grads(prev_layer_grad)\n",
    "        bias_gradients = np.sum(prev_layer_grad, axis=(1, 2)) # Summing over all elements of each feature map for biases\n",
    "\n",
    "        self.update_params(filter_gradients, bias_gradients) # Update filter weights and biases\n",
    "        return input_gradients\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def calc_filter_grads(self, prev_layer_grad):\n",
    "        filter_gradients = np.zeros((self.num_filters, self.last_input.shape[0], self.filter_size, self.filter_size))\n",
    "        output_rows = (self.last_input.shape[1] - prev_layer_grad.shape[1]) + 1\n",
    "        output_cols = (self.last_input.shape[2] - prev_layer_grad.shape[2]) + 1\n",
    "\n",
    "        for n in range(self.num_filters):\n",
    "            # Convolve prev_layer_grad over last_input to calculate dL/dF (loss with respect to filter)\n",
    "            filter_gradients[n] = self.filter_grads_helper(self.last_input, prev_layer_grad[n], output_rows, output_cols)\n",
    "\n",
    "        return filter_gradients\n",
    "\n",
    "    def filter_grads_helper(self, input_arr, kernel, output_rows, output_cols):\n",
    "        assert input_arr.ndim == 3, \"input_arr must be a 3D array (batch_size, input height, input width)\"\n",
    "        assert kernel.ndim == 2, \"kernel must be a 2d array (output height, output width)\"\n",
    "\n",
    "        conv_result = np.zeros((input_arr.shape[0], output_rows, output_cols))\n",
    "\n",
    "        for i in range(input_arr.shape[0]):\n",
    "            im2col_matrix = self.im2col(input_arr[i], kernel, output_rows, output_cols)\n",
    "            dot_result = np.dot(kernel.flatten(), im2col_matrix)\n",
    "            conv_result[i] = np.reshape(dot_result, (output_rows, output_cols))\n",
    "\n",
    "        return conv_result\n",
    "\n",
    "    def calc_input_grads(self, prev_layer_grad):\n",
    "        prev_layer_grad = self.pad_input(prev_layer_grad, self.filter_size - 1) # Pad input for a full convolution\n",
    "        input_gradients = np.zeros_like(self.last_input, dtype=np.float64) # Assumes last_input is a 3D numpy array\n",
    "        rot_filter_weights = np.rot90(self.filter_weights, 2, axes=(2, 3)) # Flip the filter weights' heights and widths\n",
    "\n",
    "        input_rows = (prev_layer_grad.shape[1] - rot_filter_weights.shape[2]) + 1\n",
    "        input_cols = (prev_layer_grad.shape[2] - rot_filter_weights.shape[3]) + 1\n",
    "\n",
    "        for i in range(rot_filter_weights.shape[1]): # Iterate over batch_size\n",
    "            # Convolve flipped filter_weights over prev_layer_grad to calculate dL/dX (loss with respect to input)\n",
    "            input_gradients[i] = self.convolve(prev_layer_grad, rot_filter_weights[:, i], input_rows, input_cols)\n",
    "\n",
    "        return input_gradients\n",
    "\n",
    "    def pad_input(self, input_arr, padding):\n",
    "        # input_arr should be a 3d array\n",
    "        if padding > 0:\n",
    "            # Don't pad the first dimension but pad the second two\n",
    "            input_arr = np.pad(input_arr, ((0, 0), (padding, padding), (padding, padding)))\n",
    "        return input_arr\n",
    "\n",
    "    def update_params(self, filter_grads, bias_grads):\n",
    "        self.filter_weights -= filter_grads * self.learning_rate\n",
    "        self.biases -= bias_grads * self.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good article: https://medium.com/@YasinShafiei/making-a-neural-network-fully-connected-layer-from-scratch-only-numpy-49bd7958b6f3\n",
    "\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, num_nodes, activation, learning_rate, regularization_param):\n",
    "        self.num_nodes = num_nodes # The number of nodes in the layer\n",
    "        self.activation = activation # The activation function to use, either \"relu\" or \"softmax\", softmax will only be used for the final layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weights = None # Will be initialized in the forward pass when the input size is known\n",
    "        self.biases = np.zeros((1, num_nodes))\n",
    "        self.regularization_param = regularization_param\n",
    "        self.last_input = None\n",
    "        self.last_output = None\n",
    "        self.weights_initialized = False\n",
    "\n",
    "    def forward_pass(self, input_arr): # Assumes input_arr is a flattened vector of shape (1, x)\n",
    "        if not self.weights_initialized:\n",
    "            self.weights = np.random.randn(input_arr.shape[1], self.num_nodes) * np.sqrt(2.0 / input_arr.shape[1])\n",
    "            self.weights_initialized = True\n",
    "\n",
    "        self.last_input = input_arr\n",
    "        output = np.dot(input_arr, self.weights) + self.biases\n",
    "        if self.activation == \"relu\":\n",
    "            output = self.relu(output)\n",
    "        if self.activation == \"softmax\":\n",
    "            # print(\"output before softmax\", output)\n",
    "            output = self.softmax(output)\n",
    "\n",
    "        self.last_output = output # Store output for backpropagation\n",
    "        return output\n",
    "\n",
    "    def backward_pass(self, prev_layer_grad):\n",
    "        if self.activation == \"relu\":\n",
    "            prev_layer_grad *= self.relu_derivative(self.last_output)\n",
    "\n",
    "        # If the activation is softmax we don't need to do anything as we assume that prev_layer_grad\n",
    "        # is already the difference between the softmax output and the true labels (s - y)\n",
    "\n",
    "        input_gradients = np.dot(prev_layer_grad, self.weights.T)\n",
    "        weight_gradients = np.dot(self.last_input.T, prev_layer_grad)\n",
    "        bias_gradients = np.sum(prev_layer_grad, axis=0, keepdims=True)\n",
    "\n",
    "        # Add in l2 regularization. Normally you add a term proportional to the sum of the squared weights:\n",
    "        # (lambda/2) * sum_i(weights[i]^2). But here we add the regularization term to the gradients so we\n",
    "        # need to take the gradient of that equation to get lambda * sum_i(weights[i]).\n",
    "        weight_gradients += self.regularization_param * self.weights\n",
    "\n",
    "        self.update_params(weight_gradients, bias_gradients)\n",
    "        return input_gradients\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        assert x.shape[0] == 1\n",
    "        assert x.ndim == 2\n",
    "\n",
    "        numerator = np.exp(x - np.max(x, axis=1))\n",
    "        denominator = np.sum(numerator, axis=1)\n",
    "        return numerator / denominator\n",
    "\n",
    "    def update_params(self, weight_grads, bias_grads):\n",
    "        weight_grads = np.clip(weight_grads, -1.0, 1.0)\n",
    "        bias_grads = np.clip(bias_grads, -1.0, 1.0)\n",
    "\n",
    "        self.weights -= weight_grads * self.learning_rate\n",
    "        self.biases -= bias_grads * self.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    "    def __init__(self, filter_size, stride_size):\n",
    "        assert filter_size == stride_size, \"filter size and stride size must be the same\"\n",
    "        self.filter_size = filter_size\n",
    "        self.stride_size = stride_size\n",
    "        self.last_input = None\n",
    "        self.indices = [] # An array that lists the indices of the maxes\n",
    "\n",
    "    def convolve(self, input_arr, kernel_size, step_size, n):\n",
    "        assert input_arr.ndim == 2, \"input_arr must be a 2d array\"\n",
    "\n",
    "        output_shape = (input_arr.shape[0] - kernel_size) // step_size + 1\n",
    "        windows = view_as_windows(input_arr, kernel_size, step_size)\n",
    "        maxes = np.max(windows, axis=(2, 3))\n",
    "        im2row_matrix = windows.reshape(output_shape ** 2, kernel_size ** 2)\n",
    "\n",
    "        col_indices = np.argmax(im2row_matrix, axis=1)\n",
    "        row_indices = np.arange(im2row_matrix.shape[0])\n",
    "        n_indices = np.full(col_indices.size, n)\n",
    "        old_indices = list(zip(n_indices, row_indices, col_indices))\n",
    "        new_indices = []\n",
    "\n",
    "        for n, row_idx, col_idx in old_indices:\n",
    "            row_offset = (row_idx // output_shape) * kernel_size\n",
    "            col_offset = (row_idx % output_shape) * kernel_size\n",
    "            row = row_offset + (col_idx // kernel_size)\n",
    "            col = col_offset + (col_idx % kernel_size)\n",
    "            new_indices.append((n, row, col))\n",
    "\n",
    "        return maxes, new_indices\n",
    "\n",
    "\n",
    "    def forward_pass(self, input_arr):\n",
    "        assert input_arr.ndim == 3, \"input_arr must be a three dimensional array (output of convolutional layer)\"\n",
    "        assert input_arr.shape[1] == input_arr.shape[2], \"input_arr must only contain square feature maps/images\"\n",
    "\n",
    "        self.indices = [] # Reset the indices list\n",
    "        self.last_input = input_arr\n",
    "        pooling_output = np.zeros((input_arr.shape[0], input_arr.shape[1] // self.filter_size, input_arr.shape[2] // self.filter_size))\n",
    "\n",
    "        for n in range(pooling_output.shape[0]):\n",
    "            maxes, new_indices = self.convolve(input_arr[n], self.filter_size, self.stride_size, n)\n",
    "            pooling_output[n] = maxes\n",
    "            self.indices.extend(new_indices)\n",
    "\n",
    "        return pooling_output\n",
    "\n",
    "\n",
    "    def backward_pass(self, prev_layer_grad):\n",
    "        input_indices = np.zeros_like(self.last_input, dtype=np.float64)\n",
    "\n",
    "        for index in self.indices:\n",
    "            input_indices[index[0], index[1], index[2]] = 1\n",
    "\n",
    "        grad_repeat_cols = np.repeat(prev_layer_grad, self.filter_size, axis=2)\n",
    "        grad_repeat_rows = np.repeat(grad_repeat_cols, self.filter_size, axis=1)\n",
    "\n",
    "        # Pad gradients in case stride size and input size weren't compatible\n",
    "        if grad_repeat_rows.shape != input_indices.shape:\n",
    "            pad_margin = input_indices.shape[1] % self.stride_size\n",
    "            grad_repeat_rows = np.pad(grad_repeat_rows, ((0, 0), (0, pad_margin), (0, pad_margin)))\n",
    "\n",
    "        input_gradients = input_indices * grad_repeat_rows\n",
    "        return input_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel:\n",
    "    def __init__(self):\n",
    "        self.conv_layer1 = ConvLayer(num_filters=64, filter_size=3, learning_rate=0.01)\n",
    "        self.mp_layer1 = MaxPoolingLayer(filter_size=2, stride_size=2)\n",
    "        self.conv_layer2 = ConvLayer(num_filters=64, filter_size=3, learning_rate=0.01)\n",
    "        self.mp_layer2 = MaxPoolingLayer(filter_size=2, stride_size=2)\n",
    "        self.fc_layer1 = FullyConnectedLayer(num_nodes=400, activation=\"relu\", learning_rate=0.001, regularization_param=0.1)\n",
    "        self.fc_layer2 = FullyConnectedLayer(num_nodes=100, activation=\"relu\", learning_rate=0.001, regularization_param=0.1)\n",
    "        self.fc_layer3 = FullyConnectedLayer(num_nodes=10, activation=\"softmax\", learning_rate=0.001, regularization_param=0.1)\n",
    "        self.mp_layer2_shape = None # Will be initialized in forward pass and used in backward pass\n",
    "\n",
    "    def forward_pass(self, input_image):\n",
    "        conv_layer1_output = self.conv_layer1.forward_pass(input_image)\n",
    "        mp_layer1_output = self.mp_layer1.forward_pass(conv_layer1_output)\n",
    "        conv_layer2_output = self.conv_layer2.forward_pass(mp_layer1_output)\n",
    "        mp_layer2_output = self.mp_layer2.forward_pass(conv_layer2_output)\n",
    "        self.mp_layer2_shape = mp_layer2_output.shape\n",
    "        flattened_output = self.flatten(mp_layer2_output)\n",
    "        fc_layer1_output = self.fc_layer1.forward_pass(flattened_output)\n",
    "        fc_layer2_output = self.fc_layer2.forward_pass(fc_layer1_output)\n",
    "        fc_layer3_output = self.fc_layer3.forward_pass(fc_layer2_output)\n",
    "\n",
    "        return fc_layer3_output\n",
    "\n",
    "    def flatten(self, array):\n",
    "        # Assumes array is the output of a max pooling layer and is therefore a 3d array\n",
    "        flattened_array = np.reshape(array, (1, array.shape[0] * array.shape[1] * array.shape[2]))\n",
    "        return flattened_array\n",
    "\n",
    "    def backward_pass(self, output_grad):\n",
    "        fc_layer3_backward_output = self.fc_layer3.backward_pass(output_grad)\n",
    "        fc_layer2_backward_output = self.fc_layer2.backward_pass(fc_layer3_backward_output)\n",
    "        fc_layer1_backward_output = self.fc_layer1.backward_pass(fc_layer2_backward_output)\n",
    "        resized_output = np.reshape(fc_layer1_backward_output, self.mp_layer2_shape)\n",
    "        mp_layer2_backward_output = self.mp_layer2.backward_pass(resized_output)\n",
    "        conv_layer2_backward_output = self.conv_layer2.backward_pass(mp_layer2_backward_output)\n",
    "        mp_layer1_backward_output = self.mp_layer1.backward_pass(conv_layer2_backward_output)\n",
    "        conv_layer1_backward_output = self.conv_layer1.backward_pass(mp_layer1_backward_output)\n",
    "\n",
    "        return conv_layer1_backward_output\n",
    "\n",
    "    def train(self, train_images, train_labels, epochs, val_images=None, val_labels=None):\n",
    "        for i in range(epochs):\n",
    "            correct, total = 0, 0\n",
    "            counter = 0\n",
    "            total_loss = 0\n",
    "            for train_image, train_label in zip(train_images, train_labels): # Use SGD instead of mini-batch gradient descent because it's simpler\n",
    "                # Perform forward pass\n",
    "                train_image = np.reshape(train_image, (1, train_image.shape[0], train_image.shape[1]))\n",
    "                forward_output = self.forward_pass(train_image)\n",
    "\n",
    "                # Calculate categorical cross-entropy loss\n",
    "                epsilon = 1e-10 # To avoid getting infinity when taking the log of zero\n",
    "                loss = -np.sum(train_label * np.log(forward_output + epsilon))\n",
    "                total_loss += loss\n",
    "\n",
    "                # Update correct and total counters\n",
    "                predicted_label = np.argmax(forward_output, axis=1) # forward_output is shape (1, 10)\n",
    "                true_label = np.argmax(train_label) # label is shape (10, )\n",
    "                correct += 1 if true_label == predicted_label else 0\n",
    "                total += 1\n",
    "\n",
    "                # Perform backward pass\n",
    "                output_grad = forward_output - train_label # softmax output - label (s - y)\n",
    "                input_image_grad = self.backward_pass(output_grad)\n",
    "\n",
    "                if counter % 1000 == 0:\n",
    "                    print(f\"Epoch {i}, iteration {counter}\")\n",
    "\n",
    "                counter += 1\n",
    "\n",
    "            # Calculate validation and train accuracy\n",
    "            if val_images is not None and val_labels is not None:\n",
    "                val_accuracy = self.predict(val_images, val_labels)\n",
    "            train_accuracy = correct / total\n",
    "\n",
    "            # Log statistics\n",
    "            print(f\"Epoch {i+1}/{epochs}, Loss: {total_loss}, Accuracy: {train_accuracy}, Elapsed time in seconds: {time.time() - start_time}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    def predict(self, val_images, val_labels):\n",
    "        correct, total = 0, 0\n",
    "        for val_image, val_label in zip(val_images, val_labels):\n",
    "            val_image = np.reshape(val_image, (1, val_image.shape[0], val_image.shape[1]))\n",
    "            forward_output = self.forward_pass(val_image)\n",
    "\n",
    "            predicted_label = np.argmax(forward_output, axis=1) # forward_output is shape (1, 10)\n",
    "            true_label = np.argmax(val_label) # label is shape (10, )\n",
    "            correct += 1 if true_label == predicted_label else 0\n",
    "            total += 1\n",
    "\n",
    "        accuracy = correct / total\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CreateModel()\n",
    "subset_data = train_data\n",
    "subset_labels = np.zeros((subset_data.shape[0], 10)) # Make subset labels one hot vectors\n",
    "for i in range(subset_labels.shape[0]):\n",
    "    label = train_labels[i][0]\n",
    "    subset_labels[i][label] = 1\n",
    "\n",
    "subset_val_data = val_data\n",
    "subset_val_labels = np.zeros((subset_val_data.shape[0], 10)) # Make validation labels one hot\n",
    "for i in range(subset_val_data.shape[0]):\n",
    "    label = val_labels[i][0]\n",
    "    subset_val_labels[i][label] = 1\n",
    "\n",
    "model.train(subset_data, subset_labels, 3, subset_val_data, subset_val_labels)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
